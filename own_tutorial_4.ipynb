{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralthreads\n",
    "[medium](https://neuralthreads.medium.com/i-was-not-satisfied-by-any-deep-learning-tutorials-online-37c5e9f4bea1)\n",
    "\n",
    "## Chapter 2 — Optimizers\n",
    "\n",
    "### 2.1 What is SGD or Stochastic Gradient Descent?\n",
    "\n",
    "> First post (own_tutorial_1.ipynb).\n",
    "> Second post (own_tutorial_2.ipynb).\n",
    "> Previous post (own_tutorial_3.ipynb).\n",
    "\n",
    "\n",
    "**Gradient Descent — Trick developed in 1847 is now the first step in the field of Neural Networks**\n",
    "\n",
    "Step by step implementation with animation for better understanding.\n",
    "\n",
    "This post is divided into 3 sections\n",
    "\n",
    "1. SGD in 1 Variable function\n",
    "2. SGD Animation for 1 variable function\n",
    "3. SGD in multi-variable function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SGD in 1 variable function**\n",
    "Suppose, we need to find the minima of this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align}\n",
       "    y = f(x) = x - x^{3} \\\\\n",
       "    \\text{We will use calculus for this. We will simply take the derivative of the functions and equate it to 0.}\\\\\n",
       "    \\frac{dy}{dx} = f\\'(x) = 1 - 3x^{2} \\\\\n",
       "    f^\\prime(x) = 0 \\\\\n",
       "    \\text{Which gives us} \\\\\n",
       "    1 - 3x^{2} = 0 \\, \\Rightarrow \\, x = \\pm \\frac{1}{\\sqrt{3}} \\\\\n",
       "    \\text{The points will either be minima, or maxima, or points of inflection.} \\\\\n",
       "    \\text{For minima, the second derivative should be positive.} \\\\\n",
       "    \\frac{d^{2}y}{dx^{2}} = {f}^\\prime\\prime(x) = -6x \\\\\n",
       "\n",
       "\\end{align}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align}\n",
    "    y = f(x) = x - x^{3} \\\\\n",
    "    \\text{We will use calculus for this. We will simply take the derivative of the functions and equate it to 0.}\\\\\n",
    "    \\frac{dy}{dx} = f\\'(x) = 1 - 3x^{2} \\\\\n",
    "    f^\\prime(x) = 0 \\\\\n",
    "    \\text{Which gives us} \\\\\n",
    "    1 - 3x^{2} = 0 \\, \\Rightarrow \\, x = \\pm \\frac{1}{\\sqrt{3}} \\\\\n",
    "    \\text{The points will either be minima, or maxima, or points of inflection.} \\\\\n",
    "    \\text{For minima, the second derivative should be positive.} \\\\\n",
    "    \\frac{d^{2}y}{dx^{2}} = {f}^\\prime^\\prime(x) = -6x \\\\\n",
    "\n",
    "\\end{align}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
