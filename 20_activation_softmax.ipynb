{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralthreads\n",
    "[medium](https://neuralthreads.medium.com/i-was-not-satisfied-by-any-deep-learning-tutorials-online-37c5e9f4bea1)\n",
    "\n",
    "## Chapter 3 — Activation functions and their derivatives\n",
    "\n",
    "Softmax function — It is frustrating that everyone talks about it but very few talk about its Jacobian\n",
    "\n",
    "### 3.7 What is the Softmax function and how to compute its Jacobian?\n",
    "\n",
    "This is the most important post in the third chapter. In this post, we will talk about the Softmax activation function and how to compute its Jacobian.\n",
    "\n",
    "This is the definition of the Softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    y = softmax(x) = f(x) = \\dfrac{e^{x_i}}{\\sum_{i=1}^{i=n}e^{x_i}}\n",
       "    \\\\\n",
       "    \\text{Suppose we have 'x'}\\\\\n",
       "    \\\\\n",
       "    x = \n",
       "    \\begin{bmatrix}\n",
       "    x_1\\\\\n",
       "    x_2\\\\\n",
       "    x_3\\\\\n",
       "    x_4\n",
       "    \\end{bmatrix} \\\\\n",
       "    \\\\\n",
       "    \\text{Then 'y' is}\\\\\n",
       "    \\\\\n",
       "    \\Rightarrow y = \n",
       "    \\begin{bmatrix}\n",
       "    y_1\\\\\n",
       "    y_2\\\\\n",
       "    y_3\\\\\n",
       "    y_4\n",
       "    \\end{bmatrix} = softmax(x) = f(x) =  \n",
       "    \\begin{bmatrix}\n",
       "    \\dfrac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
       "    \\\\\n",
       "    \\dfrac{e^{x_2}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
       "    \\\\\n",
       "    \\dfrac{e^{x_3}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
       "    \\\\\n",
       "    \\dfrac{e^{x_4}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
       "    \\end{bmatrix} \\\\\n",
       "    \\\\\n",
       "    \\text{We can easily define the Softmax function in Python by doing this reduction} \\\\\n",
       "    \\\\\n",
       "    \\begin{bmatrix}\n",
       "    \\dfrac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
       "    \\\\\n",
       "    \\dfrac{e^{x_2}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
       "    \\\\\n",
       "    \\dfrac{e^{x_3}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
       "    \\\\\n",
       "    \\dfrac{e^{x_4}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
       "    \\end{bmatrix}  = \n",
       "    \\begin{bmatrix} \\\\\n",
       "    e^{x_1} \\\\\n",
       "    e^{x_2} \\\\\n",
       "    e^{x_3} \\\\\n",
       "    e^{x_4} \\\\    \n",
       "    \\end{bmatrix} \\big/ (e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}) \\Rightarrow \\\\\n",
       "    \\\\\n",
       "    \\Rightarrow e^{x} / sum(e^{x})       \n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    y = softmax(x) = f(x) = \\dfrac{e^{x_i}}{\\sum_{i=1}^{i=n}e^{x_i}}\n",
    "    \\\\\n",
    "    \\text{Suppose we have 'x'}\\\\\n",
    "    \\\\\n",
    "    x = \n",
    "    \\begin{bmatrix}\n",
    "    x_1\\\\\n",
    "    x_2\\\\\n",
    "    x_3\\\\\n",
    "    x_4\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\text{Then 'y' is}\\\\\n",
    "    \\\\\n",
    "    \\Rightarrow y = \n",
    "    \\begin{bmatrix}\n",
    "    y_1\\\\\n",
    "    y_2\\\\\n",
    "    y_3\\\\\n",
    "    y_4\n",
    "    \\end{bmatrix} = softmax(x) = f(x) =  \n",
    "    \\begin{bmatrix}\n",
    "    \\dfrac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
    "    \\\\\n",
    "    \\dfrac{e^{x_2}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
    "    \\\\\n",
    "    \\dfrac{e^{x_3}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
    "    \\\\\n",
    "    \\dfrac{e^{x_4}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
    "    \\end{bmatrix} \\\\\n",
    "    \\\\\n",
    "    \\text{We can easily define the Softmax function in Python by doing this reduction} \\\\\n",
    "    \\\\\n",
    "    \\begin{bmatrix}\n",
    "    \\dfrac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
    "    \\\\\n",
    "    \\dfrac{e^{x_2}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
    "    \\\\\n",
    "    \\dfrac{e^{x_3}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
    "    \\\\\n",
    "    \\dfrac{e^{x_4}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}} \\\\\n",
    "    \\end{bmatrix}  = \n",
    "    \\begin{bmatrix} \\\\\n",
    "    e^{x_1} \\\\\n",
    "    e^{x_2} \\\\\n",
    "    e^{x_3} \\\\\n",
    "    e^{x_4} \\\\    \n",
    "    \\end{bmatrix} \\big/ (e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}) \\Rightarrow \\\\\n",
    "    \\\\\n",
    "    \\Rightarrow e^{x} / sum(e^{x})       \n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                             # importing NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "def softmax(x):                                # Softmax\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the most important question is how to compute its Derivative/Jacobian?\n",
    "\n",
    "Unlike the Sigmoid activation function or any other previous activation function, **we don’t have a situation like this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    y_1 = f_1(x_1) \\\\\n",
       "    \\\\\n",
       "    y_2 = f_2(x_2) \\\\\n",
       "    \\\\\n",
       "    y_3 = f_3(x_3) \\\\\n",
       "    \\\\\n",
       "    y_4 = f_4(x_4) \\\\\n",
       "    \\\\\n",
       "    \\text{Instead, we have a situation like this.}\\\\\n",
       "    \\\\\n",
       "    y_1 = f_1(x_1,x_2,x_3,x_4) \\\\\n",
       "    \\\\\n",
       "    y_2 = f_2(x_1,x_2,x_3,x_4) \\\\\n",
       "    \\\\\n",
       "    y_3 = f_3(x_1,x_2,x_3,x_4) \\\\\n",
       "    \\\\\n",
       "    y_4 = f_4(x_1,x_2,x_3,x_4) \\\\\n",
       "    \\\\    \n",
       "    \\text{In such a case, we use something called Jacobians.} \\\\\n",
       "    \\text{Jacobian in a very simple language is a collection of partial derivatives.}\\\\\n",
       "    \\text{So, the Jacobian 'J' for the Softmax function is:}\\\\\n",
       "    \\\\\n",
       "    \\newcommand{\\arraystretch}{2}\n",
       "    J = \\frac{\\partial (y_1,y_2,y_3,y_4)}{\\partial (x_1,x_2,x_3,x_4)} = \n",
       "    \\begin{bmatrix*}\n",
       "      \\frac{\\partial y_1}{\\partial x_1}   & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} & \\frac{\\partial y_1}{\\partial x_4}  \\\\\n",
       "      \\frac{\\partial y_2}{\\partial x_1}   & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_3} & \\frac{\\partial y_2}{\\partial x_4} \\\\\n",
       "      \\frac{\\partial y_3}{\\partial x_1}   & \\frac{\\partial y_3}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_3} & \\frac{\\partial y_3}{\\partial x_4} \\\\\n",
       "      \\frac{\\partial y_4}{\\partial x_1}   & \\frac{\\partial y_4}{\\partial x_2} & \\frac{\\partial y_4}{\\partial x_3} & \\frac{\\partial y_4}{\\partial x_4} \\\\\n",
       "    \\end{bmatrix*}\\\\\n",
       "    \\newcommand{\\arraystretch}{1}\n",
       "    \\\\\n",
       "    \\text{Let us start finding each term in this Jacobian.}\\\\\n",
       "    \\text{Starting with the first term, i.e.,} \\\\\n",
       "    \\\\\n",
       "    \\frac{\\partial y_1}{\\partial x_1}\n",
       "    \\\\\n",
       "    \\\\\n",
       "    y_1 = \\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}}\n",
       "    \\\\\n",
       "    \\\\\n",
       "    \\frac{\\partial y_1}{\\partial x_1} = \\frac{\\partial( \\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}})}{\\partial x_1} \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = \\frac{e^{x_1}(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}) - e^{x_1}(e^{x_1})}\n",
       "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2} \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = \\frac{e^{2x_1} + e^{x_1} (e^{x_2} + e^{x_3} + e^{x_4}) - e^{2x_1}}\n",
       "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2} \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = \\frac{e^{x_1} (e^{x_2} + e^{x_3} + e^{x_4})}\n",
       "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2} \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = (\\frac{e^{x_1}}{{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}}})\n",
       "(\\frac{e^{x_2} + e^{x_3} + e^{x_4}}{{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}}}) \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = y_1\n",
       "(\\frac{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4} - e^{x_1}}{{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}}}) \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = y_1\n",
       "(1 - \\frac{e^{x_1}}{{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}}})\\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = y_1 ( 1 - y_1)\n",
       "\n",
       "    \\\\\n",
       "\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    y_1 = f_1(x_1) \\\\\n",
    "    \\\\\n",
    "    y_2 = f_2(x_2) \\\\\n",
    "    \\\\\n",
    "    y_3 = f_3(x_3) \\\\\n",
    "    \\\\\n",
    "    y_4 = f_4(x_4) \\\\\n",
    "    \\\\\n",
    "    \\text{Instead, we have a situation like this.}\\\\\n",
    "    \\\\\n",
    "    y_1 = f_1(x_1,x_2,x_3,x_4) \\\\\n",
    "    \\\\\n",
    "    y_2 = f_2(x_1,x_2,x_3,x_4) \\\\\n",
    "    \\\\\n",
    "    y_3 = f_3(x_1,x_2,x_3,x_4) \\\\\n",
    "    \\\\\n",
    "    y_4 = f_4(x_1,x_2,x_3,x_4) \\\\\n",
    "    \\\\    \n",
    "    \\text{In such a case, we use something called Jacobians.} \\\\\n",
    "    \\text{Jacobian in a very simple language is a collection of partial derivatives.}\\\\\n",
    "    \\text{So, the Jacobian 'J' for the Softmax function is:}\\\\\n",
    "    \\\\\n",
    "    \\newcommand{\\arraystretch}{2}\n",
    "    J = \\frac{\\partial (y_1,y_2,y_3,y_4)}{\\partial (x_1,x_2,x_3,x_4)} = \n",
    "    \\begin{bmatrix*}\n",
    "      \\frac{\\partial y_1}{\\partial x_1}   & \\frac{\\partial y_1}{\\partial x_2} & \\frac{\\partial y_1}{\\partial x_3} & \\frac{\\partial y_1}{\\partial x_4}  \\\\\n",
    "      \\frac{\\partial y_2}{\\partial x_1}   & \\frac{\\partial y_2}{\\partial x_2} & \\frac{\\partial y_2}{\\partial x_3} & \\frac{\\partial y_2}{\\partial x_4} \\\\\n",
    "      \\frac{\\partial y_3}{\\partial x_1}   & \\frac{\\partial y_3}{\\partial x_2} & \\frac{\\partial y_3}{\\partial x_3} & \\frac{\\partial y_3}{\\partial x_4} \\\\\n",
    "      \\frac{\\partial y_4}{\\partial x_1}   & \\frac{\\partial y_4}{\\partial x_2} & \\frac{\\partial y_4}{\\partial x_3} & \\frac{\\partial y_4}{\\partial x_4} \\\\\n",
    "    \\end{bmatrix*}\\\\\n",
    "    \\newcommand{\\arraystretch}{1}\n",
    "    \\\\\n",
    "    \\text{Let us start finding each term in this Jacobian.}\\\\\n",
    "    \\text{Starting with the first term, i.e.,} \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial y_1}{\\partial x_1}\n",
    "    \\\\\n",
    "    \\\\\n",
    "    y_1 = \\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}}\n",
    "    \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial y_1}{\\partial x_1} = \\frac{\\partial( \\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}})}{\\partial x_1} \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = \\frac{e^{x_1}(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}) - e^{x_1}(e^{x_1})}\n",
    "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2} \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = \\frac{e^{2x_1} + e^{x_1} (e^{x_2} + e^{x_3} + e^{x_4}) - e^{2x_1}}\n",
    "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2} \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = \\frac{e^{x_1} (e^{x_2} + e^{x_3} + e^{x_4})}\n",
    "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2} \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = (\\frac{e^{x_1}}{{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}}})\n",
    "(\\frac{e^{x_2} + e^{x_3} + e^{x_4}}{{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}}}) \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = y_1\n",
    "(\\frac{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4} - e^{x_1}}{{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}}}) \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = y_1\n",
    "(1 - \\frac{e^{x_1}}{{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}}})\\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow \\frac{\\partial y_1}{\\partial x_1} = y_1 ( 1 - y_1)\n",
    "\n",
    "    \\\\\n",
    "\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
       "    \\\\\n",
       "    \\frac{\\partial y_1}{\\partial x_2} = \\frac{\\partial(\\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}})}{\\partial x_2} \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow \\frac{\\partial y_1}{\\partial x_2} = \\frac{-e^{x_1}e^{x_2}}\n",
       "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2} \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow \\frac{\\partial y_1}{\\partial x_2} = - (\\frac{e^{x_1}}\n",
       "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2})\n",
       "(\\frac{e^{x_2}}\n",
       "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2})\\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow \\frac{\\partial y_1}{\\partial x_2} = - y_1 y_2\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    \\frac{\\partial y_1}{\\partial x_2} \\\\\n",
    "    \\\\\n",
    "    \\frac{\\partial y_1}{\\partial x_2} = \\frac{\\partial(\\frac{e^{x_1}}{e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4}})}{\\partial x_2} \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow \\frac{\\partial y_1}{\\partial x_2} = \\frac{-e^{x_1}e^{x_2}}\n",
    "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2} \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow \\frac{\\partial y_1}{\\partial x_2} = - (\\frac{e^{x_1}}\n",
    "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2})\n",
    "(\\frac{e^{x_2}}\n",
    "{(e^{x_1} + e^{x_2} + e^{x_3} + e^{x_4})^2})\\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow \\frac{\\partial y_1}{\\partial x_2} = - y_1 y_2\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding every term in the Jacobian, we have a symmetric matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    \\newcommand{\\arraystretch}{2}\n",
       "    J =  \n",
       "    \\begin{bmatrix*}\n",
       "      y_1(1-y_1)    & -y_1 y_2      & -y_1 y_3      & -y_1 y_4   \\\\\n",
       "      -y_1 y_2      & y_2(1-y_2)    & -y_2 y_3      & -y_2 y_4   \\\\\n",
       "      -y_1 y_3      & -y_2 y_3      & y_3(1-y_3)    & -y_3 y_4   \\\\\n",
       "      -y_1 y_4      & -y_2 y_4      & -y_3 y_4      & y_4(1-y_4) \\\\\n",
       "    \\end{bmatrix*}\\\\\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    \\newcommand{\\arraystretch}{2}\n",
    "    J =  \n",
    "    \\begin{bmatrix*}\n",
    "      y_1(1-y_1)    & -y_1 y_2      & -y_1 y_3      & -y_1 y_4   \\\\\n",
    "      -y_1 y_2      & y_2(1-y_2)    & -y_2 y_3      & -y_2 y_4   \\\\\n",
    "      -y_1 y_3      & -y_2 y_3      & y_3(1-y_3)    & -y_3 y_4   \\\\\n",
    "      -y_1 y_4      & -y_2 y_4      & -y_3 y_4      & y_4(1-y_4) \\\\\n",
    "    \\end{bmatrix*}\\\\\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reduce it to define the Softmax Jacobian in Python like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    \\newcommand{\\arraystretch}{2}\n",
       "    J =  \\begin{bmatrix*}\n",
       "      y_1 \\\\\n",
       "      y_2 \\\\\n",
       "      y_3 \\\\\n",
       "      y_4 \\\\\n",
       "    \\end{bmatrix*} * \n",
       "    \\begin{bmatrix*}\n",
       "      1-y_1   & -y_2    & -y_3     & -y_4   \\\\\n",
       "      -y_1    & 1-y_2   & -y_3     & -y_4   \\\\\n",
       "      -y_1    & -y_2    & 1-y_3    & -y_4   \\\\\n",
       "      -y_1    & -y_2    & -y_4     & 1-y_4  \\\\\n",
       "    \\end{bmatrix*}\\\\\n",
       "    \\\\\n",
       "    J =  \\begin{bmatrix*}\n",
       "      y_1 \\\\\n",
       "      y_2 \\\\\n",
       "      y_3 \\\\\n",
       "      y_4 \\\\\n",
       "    \\end{bmatrix*} * (\n",
       "    \\begin{bmatrix*}\n",
       "      1   & 0    & 0    & 0   \\\\\n",
       "      0   & 1    & 0    & 0   \\\\\n",
       "      0   & 0    & 1    & 0   \\\\\n",
       "      0   & 0    & 0    & 1  \\\\\n",
       "    \\end{bmatrix*} - \n",
       "    \\begin{bmatrix*}\n",
       "      y_1   & y_2    & y_3     & y_4   \\\\\n",
       "    \\end{bmatrix*} ) \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow J = softmax(x) * (I - softmax(x)^{T})\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    \\newcommand{\\arraystretch}{2}\n",
    "    J =  \\begin{bmatrix*}\n",
    "      y_1 \\\\\n",
    "      y_2 \\\\\n",
    "      y_3 \\\\\n",
    "      y_4 \\\\\n",
    "    \\end{bmatrix*} * \n",
    "    \\begin{bmatrix*}\n",
    "      1-y_1   & -y_2    & -y_3     & -y_4   \\\\\n",
    "      -y_1    & 1-y_2   & -y_3     & -y_4   \\\\\n",
    "      -y_1    & -y_2    & 1-y_3    & -y_4   \\\\\n",
    "      -y_1    & -y_2    & -y_4     & 1-y_4  \\\\\n",
    "    \\end{bmatrix*}\\\\\n",
    "    \\\\\n",
    "    J =  \\begin{bmatrix*}\n",
    "      y_1 \\\\\n",
    "      y_2 \\\\\n",
    "      y_3 \\\\\n",
    "      y_4 \\\\\n",
    "    \\end{bmatrix*} * (\n",
    "    \\begin{bmatrix*}\n",
    "      1   & 0    & 0    & 0   \\\\\n",
    "      0   & 1    & 0    & 0   \\\\\n",
    "      0   & 0    & 1    & 0   \\\\\n",
    "      0   & 0    & 0    & 1  \\\\\n",
    "    \\end{bmatrix*} - \n",
    "    \\begin{bmatrix*}\n",
    "      y_1   & y_2    & y_3     & y_4   \\\\\n",
    "    \\end{bmatrix*} ) \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow J = softmax(x) * (I - softmax(x)^{T})\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must have noticed that it is very similar to the derivative of the Sigmoid function but not exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_dash(x):                           # Softmax Jacobian\n",
    "    \n",
    "    I = np.eye(x.shape[0])\n",
    "    \n",
    "    return softmax(x) * (I - softmax(x).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25]\n",
      " [-1.  ]\n",
      " [ 2.3 ]\n",
      " [-0.2 ]\n",
      " [ 1.  ]]\n",
      "[[0.08468093]\n",
      " [0.02426149]\n",
      " [0.6577931 ]\n",
      " [0.05399495]\n",
      " [0.17926953]]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.25], [-1], [2.3], [-0.2], [1]])\n",
    "print(x)\n",
    "print(softmax(x))\n",
    "print(np.sum(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must have noticed that the sum of scalars in softmax ‘y’ is equal to 1.\n",
    "Why? It is obvious from the definition of the Softmax function.\n",
    "\n",
    "y1, y2, y3, y4… can be treated as probabilities or answers to a question because their sum is equal to 1. We will see more on this later when we will talk about the Categorical Cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07751007 -0.00205449 -0.05570253 -0.00457234 -0.01518071]\n",
      " [-0.00205449  0.02367287 -0.01595904 -0.00131    -0.00434935]\n",
      " [-0.05570253 -0.01595904  0.22510134 -0.0355175  -0.11792226]\n",
      " [-0.00457234 -0.00131    -0.0355175   0.05107949 -0.00967965]\n",
      " [-0.01518071 -0.00434935 -0.11792226 -0.00967965  0.14713197]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(softmax_dash(x))\n",
    "softmax_dash(x) == softmax_dash(x).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Jacobian of the Softmax function is a symmetric matrix.\n",
    "\n",
    "I hope that now you understand the Softmax function and its Jacobian.\n",
    "\n",
    "With this, the third chapter is over. In the next post, we will start the Fourth Chapter — Losses and their derivatives with Mean Square Error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
