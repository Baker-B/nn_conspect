{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralthreads\n",
    "[medium](https://neuralthreads.medium.com/i-was-not-satisfied-by-any-deep-learning-tutorials-online-37c5e9f4bea1)\n",
    "\n",
    "## Chapter 4 — Losses and their derivatives\n",
    "\n",
    "Binary cross-entropy loss — Special case of Categorical cross-entropy loss\n",
    "\n",
    "In this post, we will talk about Binary cross-entropy loss and what it means. We will also see how to compute the gradients? And how it is a special case of Categorical cross-entropy loss?\n",
    "\n",
    "### 4.4 What is Binary cross-entropy loss and how to compute the gradients?\n",
    "\n",
    "Let me ask you which of the following sentences are true for this image?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. He is RDJ.\n",
    "2. He played the role of Iron-man in MCU.\n",
    "3. He also played Jack Sparrow.\n",
    "4. He is also Sherlock Holmes.\n",
    "\n",
    "We know that the correct answers ‘y_true’ are,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "\\newcommand{\\arraystretch}{1.5}\n",
       "    y\\_true = \n",
       "    \\begin{bmatrix*}\n",
       "    1 \\\\\n",
       "    1 \\\\\n",
       "    0 \\\\\n",
       "    1\n",
       "    \\end{bmatrix*}\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "\\newcommand{\\arraystretch}{1.5}\n",
    "    y\\_true = \n",
    "    \\begin{bmatrix*}\n",
    "    1 \\\\\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    1\n",
    "    \\end{bmatrix*}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let us suppose you don’t know and you guess your answer in probabilities ‘y_pred’ as follow,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "\\newcommand{\\arraystretch}{1.5}\n",
       "    y\\_pred =  \n",
       "    \\begin{bmatrix*}\n",
       "    0.9  \\\\\n",
       "    0.95 \\\\\n",
       "    0.75  \\\\\n",
       "    0.85\n",
       "    \\end{bmatrix*}\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "\\newcommand{\\arraystretch}{1.5}\n",
    "    y\\_pred =  \n",
    "    \\begin{bmatrix*}\n",
    "    0.9  \\\\\n",
    "    0.95 \\\\\n",
    "    0.75  \\\\\n",
    "    0.85\n",
    "    \\end{bmatrix*}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two things,\n",
    "\n",
    "**First**, Each element in y_true and y_pred is an independent answer unlike the Categorical cross-entropy example because we have 4 questions in this example.\n",
    "\n",
    "**Second**, we can set a threshold value at 0.8, i.e., all values equal to or greater than 0.8 are 1 and others are 0.  \n",
    "In that case, you correctly answered 3 out of 4 questions.  \n",
    "But what if the threshold value is 0.74. In that case, you correctly answered all questions but there is still an error because the predicted value for the third question is not 0 or close to 0.\n",
    "\n",
    "Here, we will use Binary cross-entropy loss.\n",
    "\n",
    "Suppose we have true values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "\\newcommand{\\arraystretch}{1.5}\n",
       "    y\\_true = y =\n",
       "    \\begin{bmatrix*}\n",
       "    y_1 \\\\\n",
       "    y_2 \\\\\n",
       "    y_3\n",
       "    \\end{bmatrix*}\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "\\newcommand{\\arraystretch}{1.5}\n",
    "    y\\_true = y =\n",
    "    \\begin{bmatrix*}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    y_3\n",
    "    \\end{bmatrix*}\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and predicted values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "\\newcommand{\\arraystretch}{1.5}\n",
       "    y\\_pred = \\hat{y} =\n",
       "    \\begin{bmatrix*}\n",
       "    \\hat{y_1} \\\\\n",
       "    \\hat{y_2} \\\\\n",
       "    \\hat{y_3}\n",
       "    \\end{bmatrix*}\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "\\newcommand{\\arraystretch}{1.5}\n",
    "    y\\_pred = \\hat{y} =\n",
    "    \\begin{bmatrix*}\n",
    "    \\hat{y_1} \\\\\n",
    "    \\hat{y_2} \\\\\n",
    "    \\hat{y_3}\n",
    "    \\end{bmatrix*}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then Binary cross-entropy liss is calculated as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{align*}\n",
       "    BCE = - \\frac{1}{N} \\sum_{i=1}^{i=N} [y\\_true_i \\cdot log(y\\_pred_i) + (1 - y\\_true_i) \\cdot log(1 - y\\_pred_i) ]\\\\\n",
       "        \\\\\n",
       "    BCE = - \\frac{1}{N} \\sum_{i=1}^{i=N} [y_i \\cdot log(\\hat{y_i}) + (1 - y_i) \\cdot log(1 - \\hat{y_i}) ]\\\\\n",
       "        \\\\\n",
       "    \\Rightarrow BCE = - \\dfrac{1}{3} [y_1 \\cdot log(\\hat{y_1}) + (1 - y_1) \\cdot log(1 - \\hat{y_1})\\, + \\\\ \n",
       "                                        +\\, y_2 \\cdot log(\\hat{y_2}) + (1 - y_2) \\cdot log(1 - \\hat{y_2})\\, + \\\\ \n",
       "                                           +\\,  y_3 \\cdot log(\\hat{y_3}) + (1 - y_3) \\cdot log(1 - \\hat{y_3}) ] \\\\\n",
       "\\end{align*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{align*}\n",
    "    BCE = - \\frac{1}{N} \\sum_{i=1}^{i=N} [y\\_true_i \\cdot log(y\\_pred_i) + (1 - y\\_true_i) \\cdot log(1 - y\\_pred_i) ]\\\\\n",
    "        \\\\\n",
    "    BCE = - \\frac{1}{N} \\sum_{i=1}^{i=N} [y_i \\cdot log(\\hat{y_i}) + (1 - y_i) \\cdot log(1 - \\hat{y_i}) ]\\\\\n",
    "        \\\\\n",
    "    \\Rightarrow BCE = - \\dfrac{1}{3} [y_1 \\cdot log(\\hat{y_1}) + (1 - y_1) \\cdot log(1 - \\hat{y_1})\\, + \\\\ \n",
    "                                        +\\, y_2 \\cdot log(\\hat{y_2}) + (1 - y_2) \\cdot log(1 - \\hat{y_2})\\, + \\\\ \n",
    "                                           +\\,  y_3 \\cdot log(\\hat{y_3}) + (1 - y_3) \\cdot log(1 - \\hat{y_3}) ] \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily calculate Binary cross-entropy loss in Python like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                             # importing NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "def B_cross_E(y_true, y_pred):                     # BCE\n",
    "    return - np.mean(y_true * np.log(y_pred + 10**-100) + (1 - y_true) * np.log(1 - y_pred + 10**-100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    BCE = f(\\hat{y_1},\\hat{y_2},\\hat{y_3})\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    BCE = f(\\hat{y_1},\\hat{y_2},\\hat{y_3})\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, like MSE, MAE, and CE, we have a **Jacobian** for BCE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    \\newcommand{\\arraystretch}{2.5}\n",
       "    J = \\dfrac{\\partial{(BCE)}}{(\\hat{y_1},\\hat{y_2},\\hat{y_3})} =     \n",
       "    \\begin{bmatrix*}\n",
       "    \\dfrac{\\partial{(BCE)}}{\\partial(\\hat{y_1})} \\\\\n",
       "    \\dfrac{\\partial{(BCE)}}{\\partial(\\hat{y_2})} \\\\\n",
       "    \\dfrac{\\partial{(BCE)}}{\\partial(\\hat{y_3})} \n",
       "    \\end{bmatrix*}\n",
       "    \n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    \\newcommand{\\arraystretch}{2.5}\n",
    "    J = \\dfrac{\\partial{(BCE)}}{(\\hat{y_1},\\hat{y_2},\\hat{y_3})} =     \n",
    "    \\begin{bmatrix*}\n",
    "    \\dfrac{\\partial{(BCE)}}{\\partial(\\hat{y_1})} \\\\\n",
    "    \\dfrac{\\partial{(BCE)}}{\\partial(\\hat{y_2})} \\\\\n",
    "    \\dfrac{\\partial{(BCE)}}{\\partial(\\hat{y_3})} \n",
    "    \\end{bmatrix*}\n",
    "    \n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily find each term in this Jacobian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    \\newcommand{\\arraystretch}{2}\n",
       "    \\Rightarrow J =  \n",
       "    \\begin{bmatrix*}\n",
       "      - \\dfrac{1}{3} ( \\dfrac{y_1}{\\hat{y_1}} - \\dfrac{1 - y_1}{1 - \\hat{y_1}})\\\\\n",
       "      - \\dfrac{1}{3} ( \\dfrac{y_2}{\\hat{y_2}} - \\dfrac{1 - y_2}{1 - \\hat{y_2}})\\\\\n",
       "      - \\dfrac{1}{3} ( \\dfrac{y_3}{\\hat{y_3}} - \\dfrac{1 - y_3}{1 - \\hat{y_3}})\n",
       "    \\end{bmatrix*} \\Rightarrow  \\\\\n",
       "\n",
       "    \\newcommand{\\arraystretch}{1.5}\n",
       "    \\Rightarrow J =  - \\dfrac{1}{3} (\n",
       "    \\begin{bmatrix*}\n",
       "      y_{1} \\\\\n",
       "      y_{2} \\\\ \n",
       "      y_{3} \\\\\n",
       "    \\end{bmatrix*} / \n",
       "    \\begin{bmatrix*}\n",
       "      \\hat{y_1} \\\\\n",
       "      \\hat{y_2} \\\\ \n",
       "      \\hat{y_3} \\\\\n",
       "    \\end{bmatrix*} - \n",
       "    \\begin{bmatrix*}\n",
       "      1 - y_{1} \\\\\n",
       "      1 - y_{2} \\\\ \n",
       "      1 - y_{3} \\\\\n",
       "    \\end{bmatrix*} / \n",
       "    \\begin{bmatrix*}\n",
       "      1 - \\hat{y_1} \\\\\n",
       "      1 - \\hat{y_2} \\\\ \n",
       "      1 - \\hat{y_3} \\\\\n",
       "    \\end{bmatrix*}\n",
       "    ) \\Rightarrow  \\\\\n",
       "    \\\\\n",
       "  \\Rightarrow J = - \\dfrac{1}{3} ( \\frac{y\\_true}{y\\_pred} - \\frac{1 - y\\_true}{1 - y\\_pred})\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    \\newcommand{\\arraystretch}{2}\n",
    "    \\Rightarrow J =  \n",
    "    \\begin{bmatrix*}\n",
    "      - \\dfrac{1}{3} ( \\dfrac{y_1}{\\hat{y_1}} - \\dfrac{1 - y_1}{1 - \\hat{y_1}})\\\\\n",
    "      - \\dfrac{1}{3} ( \\dfrac{y_2}{\\hat{y_2}} - \\dfrac{1 - y_2}{1 - \\hat{y_2}})\\\\\n",
    "      - \\dfrac{1}{3} ( \\dfrac{y_3}{\\hat{y_3}} - \\dfrac{1 - y_3}{1 - \\hat{y_3}})\n",
    "    \\end{bmatrix*} \\Rightarrow  \\\\\n",
    "\n",
    "    \\newcommand{\\arraystretch}{1.5}\n",
    "    \\Rightarrow J =  - \\dfrac{1}{3} (\n",
    "    \\begin{bmatrix*}\n",
    "      y_{1} \\\\\n",
    "      y_{2} \\\\ \n",
    "      y_{3} \\\\\n",
    "    \\end{bmatrix*} / \n",
    "    \\begin{bmatrix*}\n",
    "      \\hat{y_1} \\\\\n",
    "      \\hat{y_2} \\\\ \n",
    "      \\hat{y_3} \\\\\n",
    "    \\end{bmatrix*} - \n",
    "    \\begin{bmatrix*}\n",
    "      1 - y_{1} \\\\\n",
    "      1 - y_{2} \\\\ \n",
    "      1 - y_{3} \\\\\n",
    "    \\end{bmatrix*} / \n",
    "    \\begin{bmatrix*}\n",
    "      1 - \\hat{y_1} \\\\\n",
    "      1 - \\hat{y_2} \\\\ \n",
    "      1 - \\hat{y_3} \\\\\n",
    "    \\end{bmatrix*}\n",
    "    ) \\Rightarrow  \\\\\n",
    "    \\\\\n",
    "  \\Rightarrow J = - \\dfrac{1}{3} ( \\frac{y\\_true}{y\\_pred} - \\frac{1 - y\\_true}{1 - y\\_pred})\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note — Here, 3 represents ‘N’, i.e., the entries in y_true and y_pred\n",
    "\n",
    "We can easily define the BCE Jacobian in Python like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def B_cross_E_grad(y_true, y_pred):           # CE Jacobian\n",
    "    N = y_true.shape[0]\n",
    "    return -(y_true / (y_pred + 10**-100) - (1 - y_true) / (1 - y_pred + 10**-100)) / N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note — 10**-100 is for stability.\n",
    "\n",
    "Let us have a look at an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n",
      "[[0.4]\n",
      " [0.5]\n",
      " [0.8]\n",
      " [0.2]]\n",
      "(4, 1)\n"
     ]
    }
   ],
   "source": [
    "y_true = np.array([[1], [0], [1], [1]])\n",
    "print(y_true)\n",
    "y_pred = np.array([[0.4], [0.5], [0.8], [0.2]])\n",
    "print(y_pred)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8605048440456026"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_cross_E(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.625 ],\n",
       "       [ 0.5   ],\n",
       "       [-0.3125],\n",
       "       [-1.25  ]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_cross_E_grad(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope now you understand what is Binary cross-entropy loss.\n",
    "\n",
    "Now, let us talk a bit about Categorical and Binary cross-entropy loss functions.\n",
    "\n",
    "A simple ‘Yes/No’ question never has an answer either ‘Yes’ or ‘No’. They both always come in compliment to each other, i.e., ‘Yes’ + ‘No’ = 1\n",
    "\n",
    "If we had only 1 question in this example of Binary cross-entropy loss, then it is a simple case of Categorical cross-entropy loss with two options, ‘Yes’ and ‘No’.\n",
    "\n",
    "But we have ‘N’ questions, so we will take mean.\n",
    "\n",
    "> Note — In Chapter 5, we will talk more about the **Sigmoid activation function and Binary cross-entropy loss function** for Backpropagation. Because, in the output of the Sigmoid function, every element is independent and is between 0 and 1 and they can be interpreted as probabilities or answers to ‘N’ number of questions.\n",
    "\n",
    "With this post, Chapter 4 — Losses and their derivatives is finished. We will be starting Chapter 5 — Diving deep in the Neural Networks with the next post in which we will talk about how Artificial Neural Networks are built which will be followed by Backpropagation, L1 and L2 penalties, Dropout, Layer Normalization, Batch training nad Validation sets and finally UCI white wine quality dataset example."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
