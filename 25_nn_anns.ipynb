{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralthreads\n",
    "[medium](https://neuralthreads.medium.com/i-was-not-satisfied-by-any-deep-learning-tutorials-online-37c5e9f4bea1)\n",
    "\n",
    "## Chapter 5 — Diving Deep in the Neural Networks\n",
    "\n",
    "Building your own Neural Network — Understand the built of a Neural Network\n",
    "\n",
    "With this post, we will be starting the fifth chapter in which we will first understand how Neural Networks are built, and then in the following posts we will go through the Backpropagation, L1 and L2 regularization, Layer Normalization, Batch Training, and much more.\n",
    "\n",
    "> Note — This post uses many things from the previous chapters. It is recommended that you have a look at the previous posts.\n",
    "\n",
    "### 5.1 Forward Feed in ANNs\n",
    "\n",
    "Forward feed simply means calculating loss or y_hat (y predicted).\n",
    "\n",
    "But first, we must understand how to make a Neural Network.\n",
    "\n",
    "Neural Networks stores information or general pattern in parameters called ‘Weights’ and ‘Biases’.\n",
    "\n",
    "Neural Networks consist of ‘Layers’ and each layer has ‘Nodes’."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Layer with Nodes](image-1.png)  \n",
    "*Layer with Nodes*\n",
    "\n",
    "Each circle is a ‘Node’ and this collection of nodes is called a ‘Layer’.\n",
    "\n",
    "Each node is represented by a scalar in Python and a collection of nodes in shape (-1, 1) forms a layer.\n",
    "\n",
    "> Note — We can also create a Neural Network where a layer is of shape (-1,) or (1, -1) but in this course, we will make layers in shape (-1, 1).\n",
    "\n",
    "Neural Networks can have as many layers and nodes as you want.\n",
    "\n",
    "For this course, We will have the following architecture, i.e., 4 layers with 5, 3, 5, and 4 nodes respectively.\n",
    "\n",
    "![4 Layers with 5, 3, 5, and 4 Nodes for the Neural Network](image-2.png)  \n",
    "*4 Layers with 5, 3, 5, and 4 Nodes for the Neural Network*\n",
    "\n",
    "The first layer is called the ‘*Input Layer*’\n",
    "\n",
    "![Input Layer](image-3.png)  \n",
    "*Input Layer*\n",
    "\n",
    "The last layer is called the ‘*Output Layer*’.\n",
    "\n",
    "![Output Layer](image-4.png)  \n",
    "*Output Layer*\n",
    "\n",
    "Layers in between them are called the ‘Hidden Layers’.\n",
    "\n",
    "![Hidden Layers](image-5.png)  \n",
    "*Hidden Layers*\n",
    "\n",
    "Layers are connected to each other via ‘Weights’.\n",
    "\n",
    "![Layers connected via weights](image-6.png)  \n",
    "*Layers connected via weights*\n",
    "\n",
    "Let us call the collection of weights ‘w1’, ‘w2’, and ‘w3’\n",
    "\n",
    "![weights ‘w1’, ‘w2’, and ‘w3’](image-7.png)  \n",
    "*weights ‘w1’, ‘w2’, and ‘w3’*\n",
    "\n",
    "And we call each weight by nodes which they are connecting.\n",
    "For example, this weight is called ‘w3₁₂’ because it connects the first node from the previous layer to the second node in the next layer.\n",
    "\n",
    "![weight w3₁₂](image-8.png)   \n",
    "*weight w3₁₂*\n",
    "\n",
    "And each node in every layer except the input layer has a bias.\n",
    "Let us call the collections of biases ‘b1’, ‘b2’, and ‘b3’.\n",
    "\n",
    "![biases](image-9.png)  \n",
    "*biases*\n",
    "\n",
    "And we call each bias by what node it is connected to.\n",
    "For example, this bias is called ‘b3₄’ because it is connected to the fourth node.\n",
    "\n",
    "![bias b3₄](image-10.png)  \n",
    "*bias b3₄*\n",
    "\n",
    "Every hidden layer and the output layer is passed through an activation function.\n",
    "\n",
    "![Activation layers](image-11.png)  \n",
    "*Activation layers*\n",
    "\n",
    "And finally, we have a layer that represents true output and a loss function.\n",
    "\n",
    "![True output and a loss function](image-12.png)  \n",
    "*True output and a loss function*\n",
    "\n",
    "Let us name each layer one by one which will help us in making this Neural Network in Python.\n",
    "\n",
    "First, we have the input Layer ‘x’. The elements will be x₁, x₂ …\n",
    "\n",
    "![Input layer ‘x’ with elements x₁, x₂…..](image-13.png)  \n",
    "*Input layer ‘x’ with elements x₁, x₂…..*\n",
    "\n",
    "Then we have the input of the hidden 1 layer calling it ‘in_hidden_1’ or ‘I_H1’. The elements will be ‘I_H1₁’, ‘I_H1₂’ …\n",
    "\n",
    "![in_hidden_1 layer ‘I_H1’ with elements ‘I_H1₁’, ‘I_H1₂’ …](image-14.png)  \n",
    "*in_hidden_1 layer ‘I_H1’ with elements ‘I_H1₁’, ‘I_H1₂’ …*\n",
    "\n",
    "Then an Activation layer for the first hidden layer.\n",
    "\n",
    "![Activation layer for the first hidden layer](image-15.png)  \n",
    "*Activation layer for the first hidden layer*\n",
    "\n",
    "Then we have the output of hidden layer 1 calling it ‘out_hidden_1’ or ‘O_H1’. The elements will be ‘O_H1₁’, ‘O_H1₂’ …\n",
    "\n",
    "![out_hidden_1 layer ‘O_H1’ with elements ‘O_H1₁’, ‘O_H1₂’ …](image-16.png)  \n",
    "*out_hidden_1 layer ‘O_H1’ with elements ‘O_H1₁’, ‘O_H1₂’ …*\n",
    "\n",
    "Then we have the input of the hidden 2 layer calling it ‘in_hidden_2’ or ‘I_H2’. The elements will be ‘I_H2₁’, ‘I_H2₂’ …\n",
    "\n",
    "![in_hidden_2 layer ‘I_H2’ with elements ‘I_H2₁’, ‘I_H2₂’ …](image-17.png)  \n",
    "*in_hidden_2 layer ‘I_H2’ with elements ‘I_H2₁’, ‘I_H2₂’ …*\n",
    "\n",
    "Then an Activation layer for the second hidden layer.\n",
    "\n",
    "![Activation layer for the second hidden layer](image-18.png)  \n",
    "*Activation layer for the second hidden layer*\n",
    "\n",
    "Then we have the output of hidden layer 2 calling it ‘out_hidden_2’ or ‘O_H2’. The elements will be ‘O_H2₁’, ‘O_H2₂’ …\n",
    "\n",
    "![out_hidden_2 layer ‘O_H2’ with elements ‘O_H2₁’, ‘O_H2₂’ …](image-19.png)  \n",
    "*out_hidden_2 layer ‘O_H2’ with elements ‘O_H2₁’, ‘O_H2₂’ …*\n",
    "\n",
    "Then we have the input for the output layer calling it ‘in_output_layer’ or ‘I_OL’. The elements will be ‘I_OL₁’, ‘I_OL₂’ …\n",
    "\n",
    "![in_ouput_layer ‘I_OL’ with elements ‘I_OL₁’, ‘I_OL₂’ …](image-20.png)  \n",
    "*in_ouput_layer ‘I_OL’ with elements ‘I_OL₁’, ‘I_OL₂’ …*\n",
    "\n",
    "Then an Activation layer for the output layer.\n",
    "\n",
    "![Activation layer for the output layer](image-21.png)  \n",
    "*Activation layer for the output layer*\n",
    "\n",
    "Then we have the ‘y_hat’ or ‘y predicted’ layer. The elements will be ‘y_hat₁’, ‘y_hat₂’ …\n",
    "\n",
    "![‘y_hat’ or ‘y_predicted’ layer with elements ‘y_hat₁’, ‘y_hat₂’ …](image-22.png)\n",
    "*‘y_hat’ or ‘y_predicted’ layer with elements ‘y_hat₁’, ‘y_hat₂’ …*\n",
    "\n",
    "Then we have a loss function.\n",
    "\n",
    "![Loss function](image-23.png)\n",
    "*Loss function*\n",
    "\n",
    "And finally, we have the true output layer calling it ‘y’. The elements will be ‘y₁’, ‘y₂’ …\n",
    "\n",
    "![true output layer ‘y’ with elements ‘y₁’, ‘y₂’ …](image-24.png)\n",
    "*true output layer ‘y’ with elements ‘y₁’, ‘y₂’ …*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Neural Network will look like this.\n",
    "\n",
    "![Our Neural Network](image-25.png)  \n",
    "*Our Neural Network*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let us start a forward feed or let us calculate the loss.\n",
    "\n",
    "> Note — We can see that the shape of each layer is (-1, 1)\n",
    "\n",
    "How will we do it? Simple, let us see each step one by one in Python.\n",
    "\n",
    "But first a few things.  \n",
    "**First**, the activation function for the first hidden layer is the ‘ReLU activation function with leak = 0.1’.  \n",
    "**Second**, the activation function for the second hidden layer and the output layer is the ‘Sigmoid activation function’.  \n",
    "**Third**, the loss function used is ‘Mean Square Error’.  \n",
    "\n",
    "![Our Neural Network detailed](image-26.png)  \n",
    "*Our Neural Network detailed*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you calculate the total number of parameters, i.e., weights and biases? It is easy to come up with a formula.\n",
    "\n",
    "So, the steps are:\n",
    "\n",
    "> Step 1 - Importing NumPy library and defining nodes in each layer  \n",
    "> Step 2 - Inputs and true Outputs  \n",
    "> Step 3 - Defining the Activation functions and the loss function  \n",
    "> Step 4 - Random initialization of weights and zero initialization of biases  \n",
    "> Step 5 - Calculating outputs of each layer  \n",
    "> Step 6 - Calculating the loss or error\n",
    "\n",
    "Let us see each step one by one.\n",
    "\n",
    "### Step 1 — Importing NumPy library and defining nodes in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                          # importing NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "input_nodes = 5                             # nodes in each layer\n",
    "hidden_1_nodes = 3\n",
    "hidden_2_nodes = 5\n",
    "output_nodes = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 — Inputs and true Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[0.52]\n",
      " [0.93]\n",
      " [0.15]\n",
      " [0.72]\n",
      " [0.61]] (5, 1)\n",
      "y\n",
      "[[0.21]\n",
      " [0.83]\n",
      " [0.87]\n",
      " [0.75]] (4, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randint(1, 100, size = (input_nodes, 1)) / 100\n",
    "print('x')                                  # Inputs\n",
    "print(x, x.shape)\n",
    "\n",
    "y = np.random.randint(1, 100, size = (output_nodes, 1)) / 100\n",
    "print('y')                                  # Outputs\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 — Defining the Activation functions and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x, leak = 0):                      # ReLU\n",
    "    return np.where(x <= 0, leak * x, x)\n",
    "\n",
    "def sig(x):                                 # Sigmoid\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def mse(y_true, y_pred):                    # MSE\n",
    "    return np.mean((y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 — Random initialization of weights and zero initialization of biases\n",
    "\n",
    "> Note — Weights are generally initialized between -1 and 1 such that the mean is 0 and the standard deviation is 1. But here we will initialize them randomly between 0 and 1. We will generate normally distributed weights in the last post of this chapter where we will talk about the UCI White Wine quality dataset. There are many other ways to initialize the weights and biases. You may refer to the literature available on the internet.\n",
    "\n",
    "But first, we will understand what will be the shape of the weight tensors.\n",
    "\n",
    "The shape of the weight tensor will be:  \n",
    "**(nodes in the next layer, nodes in the previous layer)**\n",
    "\n",
    "So for weights w1, we will have a matrix of shape (3, 5)\n",
    "\n",
    "![weights w1](image-27.png)  \n",
    "*weights w1*\n",
    "\n",
    "We can see that the first column in the weight matrix represents the outgoing weights from the first node of the previous layer and the first row represents incoming weights to the first node in the next layer.\n",
    "\n",
    "Let us take a look one more time.\n",
    "\n",
    "![w1,b1](image-28.png)\n",
    "\n",
    "The second column represents the outgoing weights from the second node in the previous layer.\n",
    "\n",
    "![second column](image-29.png)\n",
    "\n",
    "The second row represents incoming weights to the second node in the next layer.\n",
    "\n",
    "![second row](image-30.png)\n",
    "\n",
    "So, the shape of the *weight matrix* will be  \n",
    "**(nodes in the next layer, nodes in the previous layer)**  \n",
    "and the shape of *biases* will be \n",
    "**(-1, 1)** or **(nodes in the layer, 1)**\n",
    "\n",
    "So,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1\n",
      "[[0.03142919 0.63641041 0.31435598 0.50857069 0.90756647]\n",
      " [0.24929223 0.41038292 0.75555114 0.22879817 0.07697991]\n",
      " [0.28975145 0.16122129 0.92969765 0.80812038 0.63340376]] (3, 5)\n",
      "\n",
      "\n",
      "b1\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]] (3, 1)\n",
      "w2\n",
      "[[0.87146059 0.80367208 0.18657006]\n",
      " [0.892559   0.53934224 0.80744016]\n",
      " [0.8960913  0.31800347 0.11005192]\n",
      " [0.22793516 0.42710779 0.81801477]\n",
      " [0.86073058 0.00695213 0.5107473 ]] (5, 3)\n",
      "\n",
      "\n",
      "b2\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] (5, 1)\n",
      "w3\n",
      "[[0.417411   0.22210781 0.11986537 0.33761517 0.9429097 ]\n",
      " [0.32320293 0.51879062 0.70301896 0.3636296  0.97178208]\n",
      " [0.96244729 0.2517823  0.49724851 0.30087831 0.28484049]\n",
      " [0.03688695 0.60956433 0.50267902 0.05147875 0.27864646]] (4, 5)\n",
      "\n",
      "\n",
      "b3\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] (4, 1)\n"
     ]
    }
   ],
   "source": [
    "w1 = np.random.random(size = (hidden_1_nodes, input_nodes))\n",
    "b1 = np.zeros(shape = (hidden_1_nodes, 1))\n",
    "print('w1')                                 # w1\n",
    "print(w1, w1.shape)\n",
    "print('\\n')\n",
    "print('b1')                                 # b1\n",
    "print(b1, b1.shape)\n",
    "w2 = np.random.random(size = (hidden_2_nodes, hidden_1_nodes))\n",
    "b2 = np.zeros(shape = (hidden_2_nodes, 1))\n",
    "print('w2')                                 # w2\n",
    "print(w2, w2.shape)\n",
    "print('\\n')\n",
    "print('b2')                                 # b2\n",
    "print(b2, b2.shape)\n",
    "w3 = np.random.random(size = (output_nodes, hidden_2_nodes))\n",
    "b3 = np.zeros(shape = (output_nodes, 1))\n",
    "print('w3')                                 # w3\n",
    "print(w3, w3.shape)\n",
    "print('\\n')\n",
    "print('b3')                                 # b3\n",
    "print(b3, b3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 — Calculating outputs of each layer\n",
    "\n",
    "### 5.1 Calculating ‘in_hidden_1’ or ‘I_H1’\n",
    "\n",
    "To calculate outputs of each layer, we take a weighted sum like this\n",
    "\n",
    "![I_H1](image-31.png)\n",
    "\n",
    "![formulas](image-32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use matrix multiplication to reduce it so that we can implement it in Python like this\n",
    "\n",
    "![](image-33.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_hidden_1\n",
      "[[1.5751447 ]\n",
      " [0.83631317]\n",
      " [1.40828417]] (3, 1)\n"
     ]
    }
   ],
   "source": [
    "in_hidden_1 = w1.dot(x) + b1\n",
    "print('in_hidden_1')\n",
    "print(in_hidden_1, in_hidden_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Calculating ‘out_hidden_1’ or ‘O_H1’\n",
    "\n",
    "We can calculate ‘O_H1’ by passing it through the ReLU activation function with leak = 0.1\n",
    "\n",
    "![O_H1](image-34.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    O\\_H1 = relu(I\\_H1, leak = 0.1)\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    O\\_H1 = relu(I\\_H1, leak = 0.1)\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_hidden_1\n",
      "[[1.5751447 ]\n",
      " [0.83631317]\n",
      " [1.40828417]] (3, 1)\n"
     ]
    }
   ],
   "source": [
    "out_hidden_1 = relu(in_hidden_1, leak = 0.1)\n",
    "print('out_hidden_1')\n",
    "print(out_hidden_1, out_hidden_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Calculating ‘in_hidden_2’ or ‘I_H2’\n",
    "\n",
    "![I_H2](image-36.png)\n",
    "\n",
    "![Alt text](image-37.png)\n",
    "\n",
    "Or, we can do\n",
    "\n",
    "![](image-38.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_hidden_2\n",
      "[[2.30754174]\n",
      " [2.99407378]\n",
      " [1.83240834]\n",
      " [1.86822398]\n",
      " [2.08086672]] (5, 1)\n"
     ]
    }
   ],
   "source": [
    "in_hidden_2 = w2.dot(out_hidden_1) + b2\n",
    "print('in_hidden_2')\n",
    "print(in_hidden_2, in_hidden_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Calculating ‘out_hidden_2’ or ‘O_H2’\n",
    "\n",
    "![O_H2](image-39.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    O\\_H2 = sig(I\\_H2)\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    O\\_H2 = sig(I\\_H2)\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out_hidden_2\n",
      "[[0.90949972]\n",
      " [0.95230568]\n",
      " [0.86204838]\n",
      " [0.86625264]\n",
      " [0.88902957]] (5, 1)\n"
     ]
    }
   ],
   "source": [
    "out_hidden_2 = sig(in_hidden_2)\n",
    "print('out_hidden_2')\n",
    "print(out_hidden_2, out_hidden_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Calculating ‘in_output_layer’ or ‘I_OL’\n",
    "\n",
    "![](image-41.png)\n",
    "\n",
    "![](image-42.png)\n",
    "\n",
    "or, we can do this\n",
    "\n",
    "![](image-43.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_output_layer\n",
      "[[1.82521411]\n",
      " [2.5729747 ]\n",
      " [2.05763978]\n",
      " [1.33969243]] (4, 1)\n"
     ]
    }
   ],
   "source": [
    "in_output_layer = w3.dot(out_hidden_2) + b3\n",
    "print('in_output_layer')\n",
    "print(in_output_layer, in_output_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.6 Calculating ‘y_hat’\n",
    "\n",
    "![](image-44.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    \\hat{y} = sig(I\\_OL)\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    \\hat{y} = sig(I\\_OL)\n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_hat\n",
      "[[0.8611906 ]\n",
      " [0.92910189]\n",
      " [0.8867173 ]\n",
      " [0.79243936]] (4, 1)\n"
     ]
    }
   ],
   "source": [
    "y_hat = sig(in_output_layer)\n",
    "print('y_hat')\n",
    "print(y_hat, y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print true values or ‘y’ to see how different the predicted values are from true values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "[[0.21]\n",
      " [0.83]\n",
      " [0.87]\n",
      " [0.75]] (4, 1)\n"
     ]
    }
   ],
   "source": [
    "print('y')\n",
    "print(y, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 — Calculating MSE loss or error\n",
    "\n",
    "![mse](image-46.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10898773857793441"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(y, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, I hope now you understand how to build your own Neural Network.\n",
    "\n",
    "I am not using Object-Oriented programming because I want to keep things simple for this course.\n",
    "\n",
    "In the next two posts, we will see how to use Backpropagation to calculate gradients which will help us to minimize the loss by updating weights and biases with the help of some Optimizer.\n",
    "\n",
    "If you are looking for Batch training then you have to wait for the sixth post of this chapter.\n",
    "\n",
    "//-----------------------//"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
