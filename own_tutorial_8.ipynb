{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralthreads\n",
    "[medium](https://neuralthreads.medium.com/i-was-not-satisfied-by-any-deep-learning-tutorials-online-37c5e9f4bea1)\n",
    "\n",
    "## Chapter 2 — Optimizers\n",
    "\n",
    "### 2.4 How does Adagrad works?\n",
    "\n",
    "> First post (own_tutorial_1.ipynb).\n",
    "\n",
    "> Second post (own_tutorial_2.ipynb).\n",
    "\n",
    "> Third post (own_tutorial_3.ipynb).\n",
    "\n",
    "> Fourth post (own_tutorial_4.ipynb).\n",
    "\n",
    "> Fifth post (own_tutorial_5.ipynb).\n",
    "\n",
    "> Previous post (own_tutorial_6.ipynb).\n",
    "\n",
    "Adagrad stands for Adaptive Gradient. The idea is to store the square of gradients in an accumulator. The value of the accumulator is generally initialized as 0.1\n",
    "\n",
    "And we calculate update as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    update = - learning\\_rate * \\frac{gradient}{\\sqrt{accumulator} + epsilon}\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    update = - learning\\_rate * \\frac{gradient}{\\sqrt{accumulator} + epsilon}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This post is divided into 3 sections.\n",
    "\n",
    "1. Adagrad in 1 variable\n",
    "\n",
    "2. Adagrad animation for 1 variable\n",
    "\n",
    "3. Adagrad in multi-variable function\n",
    "\n",
    "## #1 Adagrad in 1 variable\n",
    "\n",
    "In this method, we store the square of gradients in an accumulator which is initialized as 0.1\n",
    "\n",
    "Adagrad algorithm in simple language is as follows:\n",
    "\n",
    "> Step 1 - Set starting point and learning rate\n",
    "\n",
    "> Step 2 - Initiate accumulator = 0.1 and set epsilon = 10**-8\n",
    "\n",
    "> Step 3 - Initiate loop\n",
    "\n",
    ">          Step 3.1 - add square of gradients to accumulator\n",
    "\n",
    ">          Step 3.2 - calculate update as stated above\n",
    "\n",
    ">          Step 3.3 - add update to point\n",
    "\n",
    "First, let us define the function and its derivative and we start from x = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    y = f(x) = x - x^{3} \\\\\n",
       "    \\frac{dy}{dx} = f'(x) = 1 - 3x^{2} \\\\  \n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    y = f(x) = x - x^{3} \\\\\n",
    "    \\frac{dy}{dx} = f'(x) = 1 - 3x^{2} \\\\  \n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "def f(x):                           # function definition                        \n",
    "    return x - x**3\n",
    "def fdash(x):                       # function derivative definition\n",
    "    return 1 - 3*(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5773502691896274"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point = -1                                   # step 1\n",
    "learning_rate = 0.05\n",
    "\n",
    "accumulator = 0.1                            # step 2\n",
    "epsilon = 10**-8\n",
    "\n",
    "for i in range(1000):                        # step 3\n",
    "    accumulator += fdash(point)**2           # step 3.1\n",
    "\n",
    "    update = - learning_rate * fdash(point) / (accumulator**0.5 + epsilon) # step 3.2\n",
    "\n",
    "    point += update                          # step 3.3\n",
    "    \n",
    "point                                        # Minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note — Because we are storing the square of gradients in the accumulator, it gets big and big with time which slows the learning. So, the learning rate is slightly high.\n",
    "\n",
    "And, we have successfully implemented Adagrad in Python\n",
    "\n",
    "## #2 Adagrad animation for better understanding\n",
    "\n",
    "Everything thing is the same as what we did earlier for the animation of the previous 3 optimizers. We will create a list to store starting point and updated points in it and will use the iᵗʰ index value for iᵗʰ frame of the animation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
