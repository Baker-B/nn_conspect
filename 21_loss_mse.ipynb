{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neuralthreads\n",
    "[medium](https://neuralthreads.medium.com/i-was-not-satisfied-by-any-deep-learning-tutorials-online-37c5e9f4bea1)\n",
    "\n",
    "## Chapter 4 — Losses and their derivatives\n",
    "\n",
    "Mean Square Error — The most used Regression loss\n",
    "\n",
    "Let us start the fourth chapter — Losses and their gradients or derivatives with Mean Square error. This error is generally used if regression problems\n",
    "\n",
    "### 4.1 What is Mean Square error and how to compute its gradients?\n",
    "\n",
    "Suppose we have true values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "\\newcommand{\\arraystretch}{1.5}\n",
       "    y\\_true = y = \n",
       "    \\begin{bmatrix*}\n",
       "    y_1 \\\\\n",
       "    y_2 \\\\\n",
       "    y_3 \n",
       "    \\end{bmatrix*}\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "\\newcommand{\\arraystretch}{1.5}\n",
    "    y\\_true = y = \n",
    "    \\begin{bmatrix*}\n",
    "    y_1 \\\\\n",
    "    y_2 \\\\\n",
    "    y_3 \n",
    "    \\end{bmatrix*}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and predicted values,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "\\newcommand{\\arraystretch}{1.5}\n",
       "    y\\_true = y = \n",
       "    \\begin{bmatrix*}\n",
       "    \\hat{y_1} \\\\\n",
       "    \\hat{y_2} \\\\\n",
       "    \\hat{y_3} \n",
       "    \\end{bmatrix*}\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "\\newcommand{\\arraystretch}{1.5}\n",
    "    y\\_pred = y = \n",
    "    \\begin{bmatrix*}\n",
    "    \\hat{y_1} \\\\\n",
    "    \\hat{y_2} \\\\\n",
    "    \\hat{y_3} \n",
    "    \\end{bmatrix*}\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then Mean Square Error is calculated as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "\\newcommand{\\arraystretch}{1.5}\n",
       "   MSE = \\dfrac{1}{N}\\sum_{i=1}^{i=N}(y\\_true_i - y\\_pred_i)^2 \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow\n",
       "   MSE = \\dfrac{1}{N}\\sum_{i=1}^{i=N}(y_i - \\hat{y_i})^2 \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow MSE = \\dfrac{1}{3}[(y_1 - \\hat{y_1})^2 + (y_2 - \\hat{y_2})^2 + (y_3 - \\hat{y_3})^2] \\\\\n",
       "\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "\\newcommand{\\arraystretch}{1.5}\n",
    "   MSE = \\dfrac{1}{N}\\sum_{i=1}^{i=N}(y\\_true_i - y\\_pred_i)^2 \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow\n",
    "   MSE = \\dfrac{1}{N}\\sum_{i=1}^{i=N}(y_i - \\hat{y_i})^2 \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow MSE = \\dfrac{1}{3}[(y_1 - \\hat{y_1})^2 + (y_2 - \\hat{y_2})^2 + (y_3 - \\hat{y_3})^2] \\\\\n",
    "\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily calculate Mean Square Error in Python like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                             # importing NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "def mse(y_true, y_pred):                     # MSE\n",
    "    return np.mean((y_true - y_pred)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we know that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    MSE = f(\\hat{y_1},\\hat{y_2},\\hat{y_3})\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    MSE = f(\\hat{y_1},\\hat{y_2},\\hat{y_3})\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, like the [**Softmax**](./20_activation_softmax.ipynb) activation function, we have a **Jacobian** for MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    \\newcommand{\\arraystretch}{2.5}\n",
       "    J = \\dfrac{\\partial{(MSE)}}{(\\hat{y_1},\\hat{y_2},\\hat{y_3})} =     \n",
       "    \\begin{bmatrix*}\n",
       "    \\dfrac{\\partial{(MSE)}}{\\partial(\\hat{y_1})} \\\\\n",
       "    \\dfrac{\\partial{(MSE)}}{\\partial(\\hat{y_2})} \\\\\n",
       "    \\dfrac{\\partial{(MSE)}}{\\partial(\\hat{y_3})} \n",
       "    \\end{bmatrix*}\n",
       "    \n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    \\newcommand{\\arraystretch}{2.5}\n",
    "    J = \\dfrac{\\partial{(MSE)}}{(\\hat{y_1},\\hat{y_2},\\hat{y_3})} =     \n",
    "    \\begin{bmatrix*}\n",
    "    \\dfrac{\\partial{(MSE)}}{\\partial(\\hat{y_1})} \\\\\n",
    "    \\dfrac{\\partial{(MSE)}}{\\partial(\\hat{y_2})} \\\\\n",
    "    \\dfrac{\\partial{(MSE)}}{\\partial(\\hat{y_3})} \n",
    "    \\end{bmatrix*}\n",
    "    \n",
    "\\end{gather*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily find each term in this Jacobian.  \\\\\\\\\\\\\\\\\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    \\newcommand{\\arraystretch}{2}\n",
       "    J =  \n",
       "    \\begin{bmatrix*}\n",
       "      y_1(1-y_1)    & -y_1 y_2      & -y_1 y_3      & -y_1 y_4   \\\\\n",
       "      -y_1 y_2      & y_2(1-y_2)    & -y_2 y_3      & -y_2 y_4   \\\\\n",
       "      -y_1 y_3      & -y_2 y_3      & y_3(1-y_3)    & -y_3 y_4   \\\\\n",
       "      -y_1 y_4      & -y_2 y_4      & -y_3 y_4      & y_4(1-y_4) \\\\\n",
       "    \\end{bmatrix*}\\\\\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    \\newcommand{\\arraystretch}{2}\n",
    "    J =  \n",
    "    \\begin{bmatrix*}\n",
    "      y_1(1-y_1)    & -y_1 y_2      & -y_1 y_3      & -y_1 y_4   \\\\\n",
    "      -y_1 y_2      & y_2(1-y_2)    & -y_2 y_3      & -y_2 y_4   \\\\\n",
    "      -y_1 y_3      & -y_2 y_3      & y_3(1-y_3)    & -y_3 y_4   \\\\\n",
    "      -y_1 y_4      & -y_2 y_4      & -y_3 y_4      & y_4(1-y_4) \\\\\n",
    "    \\end{bmatrix*}\\\\\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can reduce it to define the Softmax Jacobian in Python like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{gather*}\n",
       "    \\newcommand{\\arraystretch}{2}\n",
       "    J =  \\begin{bmatrix*}\n",
       "      y_1 \\\\\n",
       "      y_2 \\\\\n",
       "      y_3 \\\\\n",
       "      y_4 \\\\\n",
       "    \\end{bmatrix*} * \n",
       "    \\begin{bmatrix*}\n",
       "      1-y_1   & -y_2    & -y_3     & -y_4   \\\\\n",
       "      -y_1    & 1-y_2   & -y_3     & -y_4   \\\\\n",
       "      -y_1    & -y_2    & 1-y_3    & -y_4   \\\\\n",
       "      -y_1    & -y_2    & -y_4     & 1-y_4  \\\\\n",
       "    \\end{bmatrix*}\\\\\n",
       "    \\\\\n",
       "    J =  \\begin{bmatrix*}\n",
       "      y_1 \\\\\n",
       "      y_2 \\\\\n",
       "      y_3 \\\\\n",
       "      y_4 \\\\\n",
       "    \\end{bmatrix*} * (\n",
       "    \\begin{bmatrix*}\n",
       "      1   & 0    & 0    & 0   \\\\\n",
       "      0   & 1    & 0    & 0   \\\\\n",
       "      0   & 0    & 1    & 0   \\\\\n",
       "      0   & 0    & 0    & 1  \\\\\n",
       "    \\end{bmatrix*} - \n",
       "    \\begin{bmatrix*}\n",
       "      y_1   & y_2    & y_3     & y_4   \\\\\n",
       "    \\end{bmatrix*} ) \\Rightarrow \\\\\n",
       "    \\\\\n",
       "\\Rightarrow J = softmax(x) * (I - softmax(x)^{T})\n",
       "\\end{gather*}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "\\begin{gather*}\n",
    "    \\newcommand{\\arraystretch}{2}\n",
    "    J =  \\begin{bmatrix*}\n",
    "      y_1 \\\\\n",
    "      y_2 \\\\\n",
    "      y_3 \\\\\n",
    "      y_4 \\\\\n",
    "    \\end{bmatrix*} * \n",
    "    \\begin{bmatrix*}\n",
    "      1-y_1   & -y_2    & -y_3     & -y_4   \\\\\n",
    "      -y_1    & 1-y_2   & -y_3     & -y_4   \\\\\n",
    "      -y_1    & -y_2    & 1-y_3    & -y_4   \\\\\n",
    "      -y_1    & -y_2    & -y_4     & 1-y_4  \\\\\n",
    "    \\end{bmatrix*}\\\\\n",
    "    \\\\\n",
    "    J =  \\begin{bmatrix*}\n",
    "      y_1 \\\\\n",
    "      y_2 \\\\\n",
    "      y_3 \\\\\n",
    "      y_4 \\\\\n",
    "    \\end{bmatrix*} * (\n",
    "    \\begin{bmatrix*}\n",
    "      1   & 0    & 0    & 0   \\\\\n",
    "      0   & 1    & 0    & 0   \\\\\n",
    "      0   & 0    & 1    & 0   \\\\\n",
    "      0   & 0    & 0    & 1  \\\\\n",
    "    \\end{bmatrix*} - \n",
    "    \\begin{bmatrix*}\n",
    "      y_1   & y_2    & y_3     & y_4   \\\\\n",
    "    \\end{bmatrix*} ) \\Rightarrow \\\\\n",
    "    \\\\\n",
    "\\Rightarrow J = softmax(x) * (I - softmax(x)^{T})\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must have noticed that it is very similar to the derivative of the Sigmoid function but not exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_dash(x):                           # Softmax Jacobian\n",
    "    \n",
    "    I = np.eye(x.shape[0])\n",
    "    \n",
    "    return softmax(x) * (I - softmax(x).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have a look at an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.25]\n",
      " [-1.  ]\n",
      " [ 2.3 ]\n",
      " [-0.2 ]\n",
      " [ 1.  ]]\n",
      "[[0.08468093]\n",
      " [0.02426149]\n",
      " [0.6577931 ]\n",
      " [0.05399495]\n",
      " [0.17926953]]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[0.25], [-1], [2.3], [-0.2], [1]])\n",
    "print(x)\n",
    "print(softmax(x))\n",
    "print(np.sum(softmax(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must have noticed that the sum of scalars in softmax ‘y’ is equal to 1.\n",
    "Why? It is obvious from the definition of the Softmax function.\n",
    "\n",
    "y1, y2, y3, y4… can be treated as probabilities or answers to a question because their sum is equal to 1. We will see more on this later when we will talk about the Categorical Cross-entropy loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07751007 -0.00205449 -0.05570253 -0.00457234 -0.01518071]\n",
      " [-0.00205449  0.02367287 -0.01595904 -0.00131    -0.00434935]\n",
      " [-0.05570253 -0.01595904  0.22510134 -0.0355175  -0.11792226]\n",
      " [-0.00457234 -0.00131    -0.0355175   0.05107949 -0.00967965]\n",
      " [-0.01518071 -0.00434935 -0.11792226 -0.00967965  0.14713197]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(softmax_dash(x))\n",
    "softmax_dash(x) == softmax_dash(x).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the Jacobian of the Softmax function is a symmetric matrix.\n",
    "\n",
    "I hope that now you understand the Softmax function and its Jacobian.\n",
    "\n",
    "With this, the third chapter is over. In the next post, we will start the Fourth Chapter — Losses and their derivatives with Mean Square Error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
